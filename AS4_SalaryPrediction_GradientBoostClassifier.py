# Nhu Duong
#!/usr/bin/env python
# coding: utf-8

# ---
#
# _You are currently looking at **version 1.2** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-social-network-analysis/resources/yPcBs) course resource._
#
# ---

# # Assignment 4

# In[29]:


from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
import networkx as nx
import pandas as pd
import numpy as np
import pickle


# ---
#
# ## Part 1 - Random Graph Identification
#
# For the first part of this assignment you will analyze randomly generated graphs and determine which algorithm created them.

# In[30]:


P1_Graphs = pickle.load(open('A4_graphs', 'rb'))
P1_Graphs


# <br>
# `P1_Graphs` is a list containing 5 networkx graphs. Each of these graphs were generated by one of three possible algorithms:
# * Preferential Attachment (`'PA'`)
# * Small World with low probability of rewiring (`'SW_L'`)
# * Small World with high probability of rewiring (`'SW_H'`)
#
# Anaylze each of the 5 graphs and determine which of the three algorithms generated the graph.
#
# *The `graph_identification` function should return a list of length 5 where each element in the list is either `'PA'`, `'SW_L'`, or `'SW_H'`.*

# In[31]:


df = pd.DataFrame(index=P1_Graphs[0].nodes())


# In[32]:


# This Function returns histogram inorder to check if the graph is Preferential Attachment or not
# High histogram means it is PA
def degree_distribution(G):
    degrees = G.degree()
    degree_values = sorted(set(degrees.values()))
    histogram = [list(degrees.values()).count(i)/float(nx.number_of_nodes(G))
                 for i in degree_values]

    # plot to double check the PA by visual inspection
#     plt.plot(degree_values, histogram, 'o')
#     plt.xlabel('Degree')
#     plt.ylabel('Fraction of Nodes')
#     plt.xscale('log')
#     plt.yscale('log')
#     plt.show()
    return histogram


# In[ ]:


# In[33]:


# Preferential Attachment Model = small shortest path, small clustering coef
# Small World Graphs have high clustering, when have small rewiring probability.
def graph_identification():
    graph_types = []
    for G in P1_Graphs:
        clusterings = nx.average_clustering(G)

        histogram = degree_distribution(G)

        if len(histogram) > 10:
            graph_types.append("PA")
        elif clusterings < 0.1:
            graph_types.append("SW_H")
        else:
            graph_types.append("SW_L")

    return graph_types


# In[34]:


graph_identification()


# ---
#
# ## Part 2 - Company Emails
#
# For the second part of this assignment you will be workking with a company's email network where each node corresponds to a person at the company, and each edge indicates that at least one email has been sent between two people.
#
# The network also contains the node attributes `Department` and `ManagementSalary`.
#
# `Department` indicates the department in the company which the person belongs to, and `ManagementSalary` indicates whether that person is receiving a management position salary.

# In[35]:


G = nx.read_gpickle('email_prediction.txt')

print(nx.info(G))


# ### Part 2A - Salary Prediction
#
# Using network `G`, identify the people in the network with missing values for the node attribute `ManagementSalary` and predict whether or not these individuals are receiving a management position salary.
#
# To accomplish this, you will need to create a matrix of node features using networkx, train a sklearn classifier on nodes that have `ManagementSalary` data, and predict a probability of the node receiving a management salary for nodes where `ManagementSalary` is missing.
#
#
#
# Your predictions will need to be given as the probability that the corresponding employee is receiving a management position salary.
#
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
#
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).
#
# Using your trained classifier, return a series of length 252 with the data being the probability of receiving management salary, and the index being the node id.
#
#     Example:
#
#         1       1.0
#         2       0.0
#         5       0.8
#         8       1.0
#             ...
#         996     0.7
#         1000    0.5
#         1001    0.0
#         Length: 252, dtype: float64

# In[36]:


def is_manager(row, col_name):
    if row[col_name] == 0:
        val = 0
    elif row[col_name] == 1:
        val = 1
    else:
        val = None
    return val


# In[37]:


def prepare_data():
    df = pd.DataFrame(index=G.nodes())
    df['Department'] = pd.Series(nx.get_node_attributes(G, 'Department'))
    df['ManagementSalary'] = pd.Series(nx.get_node_attributes(G, 'ManagementSalary'))
    df['clustering'] = pd.Series(nx.clustering(G))
    df['degree'] = pd.Series(nx.degree(G))
    df['betweenness'] = pd.Series(nx.betweenness_centrality(G, normalized=True))
    df['degree_centerality'] = pd.Series(nx.degree_centrality(G))
    df['closeness'] = pd.Series(nx.closeness_centrality(G, normalized=True))

    return df


# In[38]:


# In[39]:


def salary_predictions():
    df = prepare_data()
    train_features = ['Department', 'clustering', 'degree',
                      'betweenness', 'degree_centerality', 'closeness']

    # TRAINING DATA
    #  clean NA values for training
    df_train = df.dropna()
    X = df_train[train_features]
    y = df_train['ManagementSalary']
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    # PREDICTION DATA
    df_prediction = df[np.isnan(df['ManagementSalary'])]
    df_prediction = df_prediction.drop(['ManagementSalary'], axis=1)

   # The ML pramaters are chosen after runing GridSearchCV to find the best values with the highest roc
    gbc = GradientBoostingClassifier(learning_rate=0.2, loss='deviance',
                                     max_depth=5,  max_features='sqrt', n_estimators=100, subsample=0.95)

    gbc.fit(X_train, y_train)
    y_pred_proba = gbc.predict_proba(X_test)

    y_pred = gbc.predict(X_test)
    # Testing roc on training set
    fpr, tpr, _ = roc_curve(y_test, y_pred)
    roc = auc(fpr, tpr)
    # PARANETER TUNING
    # print (roc)
    # parameters = {"loss": ["deviance"],
    #               "learning_rate": [0.1, 0.15, 0.2],
    #
    #               "max_depth": [3, 5, 6],
    #               "max_features": ["log2", "sqrt"],
    #
    #               "subsample": [0.95, 1.0],
    #               "n_estimators": [10, 100]
    #               }
    # grid_clf = GridSearchCV(gbc, param_grid=parameters, scoring='roc_auc')
    # # X_scaled_train = scaler.fit_transform(X_train)
    # grid_clf.fit(X_train, y_train)
    # print(grid_clf.best_params_, grid_clf.best_score_)
    y_proba = gbc.predict_proba(df_prediction)
    management_slary_predict = pd.Series(y_proba[:, 1])
    management_slary_predict.index = df_prediction.index
    return management_slary_predict


salary_predictions()


# In[ ]:


# ### Part 2B - New Connections Prediction
#
# For the last part of this assignment, you will predict future connections between employees of the network. The future connections information has been loaded into the variable `future_connections`. The index is a tuple indicating a pair of nodes that currently do not have a connection, and the `Future Connection` column indicates if an edge between those two nodes will exist in the future, where a value of 1.0 indicates a future connection.

# In[40]:


future_connections = pd.read_csv('Future_Connections.csv', index_col=0, converters={0: eval})
future_connections.head(10)


# In[42]:


def prepare_data_ft():
    df = future_connections
    # Triadic Measurements return (NodeA, NodeB, coef). use i[2] to access the coef
    df['AAI'] = [i[2] for i in nx.adamic_adar_index(G, df.index)]
    df['JCI'] = [i[2] for i in nx.jaccard_coefficient(G, df.index)]
    df['RA'] = [i[2] for i in nx.resource_allocation_index(G, df.index)]
    df['PA'] = [i[2] for i in nx.preferential_attachment(G, df.index)]
    return df


# Using network `G` and `future_connections`, identify the edges in `future_connections` with missing values and predict whether or not these edges will have a future connection.
#
# To accomplish this, you will need to create a matrix of features for the edges found in `future_connections` using networkx, train a sklearn classifier on those edges in `future_connections` that have `Future Connection` data, and predict a probability of the edge being a future connection for those edges in `future_connections` where `Future Connection` is missing.
#
#
#
# Your predictions will need to be given as the probability of the corresponding edge being a future connection.
#
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
#
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).
#
# Using your trained classifier, return a series of length 122112 with the data being the probability of the edge being a future connection, and the index being the edge as represented by a tuple of nodes.
#
#     Example:
#
#         (107, 348)    0.35
#         (542, 751)    0.40
#         (20, 426)     0.55
#         (50, 989)     0.35
#                   ...
#         (939, 940)    0.15
#         (555, 905)    0.35
#         (75, 101)     0.65
#         Length: 122112, dtype: float64

# In[46]:


def new_connections_predictions():

    df = prepare_data_ft()
    # Training DATA
    df_train = df.dropna()
    # Prediction DATA
    df_pred = df[np.isnan(df['Future Connection'])]
    df_pred = df_pred.drop(['Future Connection'], axis=1)
    # Split TRAINING data
    X = df_train.drop(['Future Connection'], axis=1)
    y = df_train['Future Connection']
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

    # The ML pramaters are chosen after runing GridSearchCV to find the best values with the highest roc
    gbc = GradientBoostingClassifier(learning_rate=0.2, loss='deviance',
                                     max_depth=5,  max_features='sqrt', n_estimators=100, subsample=0.95)

    gbc.fit(X_train, y_train)
    y_pred_proba = gbc.predict_proba(X_test)

    y_pred = gbc.predict(X_test)
    # Testing roc on training set
    fpr, tpr, _ = roc_curve(y_test, y_pred)
    roc = auc(fpr, tpr)
    y_proba = gbc.predict_proba(df_pred)
    connection_pred = pd.Series(y_proba[:, 1])
    connection_pred.index = df_pred.index
    return connection_pred


new_connections_predictions()


# In[ ]:
